{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000, loss: 1.1084\n",
      "Epoch 20/1000, loss: 1.0568\n",
      "Epoch 30/1000, loss: 1.0619\n",
      "Epoch 40/1000, loss: 1.1858\n",
      "Epoch 50/1000, loss: 1.0839\n",
      "Epoch 60/1000, loss: 1.0310\n",
      "Epoch 70/1000, loss: 1.0744\n",
      "Epoch 80/1000, loss: 1.1136\n",
      "Epoch 90/1000, loss: 1.0137\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 165\u001b[0m\n\u001b[0;32m    162\u001b[0m nn \u001b[38;5;241m=\u001b[39m NeuralNetworkWineQuality(X_train, y_train, layers)\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[1;32m--> 165\u001b[0m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;66;03m# Testing\u001b[39;00m\n\u001b[0;32m    168\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39maccuracy(X_train, y_train)\n",
      "Cell \u001b[1;32mIn[4], line 119\u001b[0m, in \u001b[0;36mNeuralNetworkWineQuality.train\u001b[1;34m(self, epochs, batch_size, learning_rate)\u001b[0m\n\u001b[0;32m    116\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m y_shuffled[i : i \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackward(X_batch, y_batch, y_pred)\n",
      "Cell \u001b[1;32mIn[4], line 73\u001b[0m, in \u001b[0;36mNeuralNetworkWineQuality.forward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz_values \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights)):\n\u001b[1;32m---> 73\u001b[0m     neurons \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivations\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases[i]\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz_values\u001b[38;5;241m.\u001b[39mappend(neurons)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "SEED = 4321\n",
    "np.random.seed(SEED)\n",
    "\n",
    "\n",
    "# Load data\n",
    "\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "X = df.drop(columns=[\"quality\"]).values\n",
    "y = df[\"quality\"].values\n",
    "N, D = X.shape\n",
    "\n",
    "y_one_hot = np.zeros((N, len(np.unique(y))))\n",
    "for i, label in enumerate(y):\n",
    "    # 3-9 -> 0-6\n",
    "    # 6 -> [0, 0, 0, 1, 0, 0, 0]\n",
    "    y_one_hot[i, int(label) - 3] = 1 \n",
    "\n",
    "outputs_num = y_one_hot.shape[1]\n",
    "inputs_num = D\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_one_hot, test_size=0.25, random_state=SEED\n",
    ")\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)  # Fit + transform for train data\n",
    "X_test = scaler.transform(X_test)  # Only transform for test data\n",
    "\n",
    "\n",
    "# Implemented neural network\n",
    "\n",
    "\n",
    "class NeuralNetworkWineQuality:\n",
    "    def __init__(self, X, y, layers):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.layers = layers\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        # Initiation of weights and biases for each layer\n",
    "        for i in range(len(layers) - 1):\n",
    "            limit = np.sqrt(1 / layers[i])\n",
    "            self.weights.append(\n",
    "                np.random.uniform(-limit, limit, (layers[i], layers[i + 1]))\n",
    "            )\n",
    "            self.biases.append(np.zeros((1, layers[i + 1])))\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward pass in the network\"\"\"\n",
    "        self.activations = [X]\n",
    "        self.z_values = []\n",
    "\n",
    "        for i in range(len(self.weights)):\n",
    "            neurons = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]\n",
    "            self.z_values.append(neurons)\n",
    "            if i < len(self.weights) - 1:\n",
    "                a = self.sigmoid(neurons)\n",
    "            else:\n",
    "                a = self.softmax(neurons)  # Softmax fot output layer\n",
    "            self.activations.append(a)\n",
    "\n",
    "        return self.activations[-1]\n",
    "\n",
    "    def backward(self, X, y, y_pred):\n",
    "        \"\"\"Backward pass in the network\"\"\"\n",
    "        m = X.shape[0]\n",
    "        dz = y_pred - y\n",
    "\n",
    "        for i in range(len(self.weights) - 1, -1, -1):\n",
    "            dw = np.dot(self.activations[i].T, dz) / m\n",
    "            db = np.sum(dz, axis=0, keepdims=True) / m\n",
    "\n",
    "            if i > 0:\n",
    "                dz = np.dot(dz, self.weights[i].T) * self.sigmoid_derivative(\n",
    "                    self.activations[i]\n",
    "                )\n",
    "\n",
    "            # Update weights and biases\n",
    "            self.weights[i] -= self.learning_rate * dw\n",
    "            self.biases[i] -= self.learning_rate * db\n",
    "\n",
    "    def train(self, epochs, batch_size, learning_rate):\n",
    "        \"\"\"Algorithm for training the network with backpropagation and mini-batch\"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.best_weights = None  # Store best weights and biases\n",
    "        self.best_biases = None\n",
    "        self.loss = self.compute_loss(self.y, self.forward(self.X))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Random permutation of the data\n",
    "            permutation = np.random.permutation(self.X.shape[0])\n",
    "            X_shuffled = self.X[permutation]\n",
    "            y_shuffled = self.y[permutation]\n",
    "\n",
    "            for i in range(0, self.X.shape[0], batch_size):\n",
    "                X_batch = X_shuffled[i : i + batch_size]\n",
    "                y_batch = y_shuffled[i : i + batch_size]\n",
    "\n",
    "                # Forward pass\n",
    "                y_pred = self.forward(X_batch)\n",
    "\n",
    "                # Backward pass\n",
    "                self.backward(X_batch, y_batch, y_pred)\n",
    "\n",
    "            # Optional: print loss every 10 epochs\n",
    "            loss = self.compute_loss(self.y, self.forward(self.X))\n",
    "            if loss < self.loss:\n",
    "                self.loss = loss\n",
    "                self.best_weights = [w.copy() for w in self.weights]\n",
    "                self.best_biases = [b.copy() for b in self.biases]\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch + 1}/{epochs}, loss: {loss:.4f}\")\n",
    "\n",
    "        self.weights = self.best_weights\n",
    "        self.biases = self.best_biases\n",
    "\n",
    "    def compute_loss(self, y, y_pred):\n",
    "        \"\"\"Cross-entropy loss function\"\"\"\n",
    "        return -np.mean(np.sum(y * np.log(y_pred + 1e-8), axis=1))\n",
    "\n",
    "    def classify(self, y):\n",
    "        \"\"\"Classify the output of the network\"\"\"\n",
    "        return np.argmax(y, axis=1).reshape(-1, 1) + 3\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        y_pred = self.forward(X)\n",
    "        y_pred_class = self.classify(y_pred)\n",
    "        y_true_class = self.classify(y)\n",
    "        return np.mean(y_pred_class == y_true_class)\n",
    "\n",
    "    def get_best_weights_and_biases(self):\n",
    "        return self.best_weights, self.best_biases\n",
    "\n",
    "    def show_predicitons(self, X, y):\n",
    "        y_pred = self.forward(X)\n",
    "        y_pred_class = self.classify(y_pred)\n",
    "        y_true_class = self.classify(y)\n",
    "        return y_pred_class, y_true_class\n",
    "\n",
    "\n",
    "# Network parameters\n",
    "layers = [inputs_num, 64, 32, outputs_num]  # Layers: input -> hidden1 -> ... -> output\n",
    "nn = NeuralNetworkWineQuality(X_train, y_train, layers)\n",
    "\n",
    "# Training\n",
    "nn.train(epochs=1000, batch_size=64, learning_rate=0.9)\n",
    "\n",
    "# Testing\n",
    "accuracy = nn.accuracy(X_train, y_train)\n",
    "print(f\"Accuracy on train set: {accuracy:.4f}\")\n",
    "accuracy = nn.accuracy(X_test, y_test)\n",
    "print(f\"Accuracy on test set: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on test set:\n",
      "      Predicted  True\n",
      "0             6     6\n",
      "1             6     6\n",
      "2             5     5\n",
      "3             4     4\n",
      "4             5     5\n",
      "...         ...   ...\n",
      "1620          6     6\n",
      "1621          6     6\n",
      "1622          8     6\n",
      "1623          7     7\n",
      "1624          6     6\n",
      "\n",
      "[1625 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "print(\"Predictions on test set:\")\n",
    "y_pred_class, y_true_class = nn.show_predicitons(X_test, y_test)\n",
    "df_test = pd.DataFrame(\n",
    "    np.concatenate((y_pred_class, y_true_class), axis=1), columns=[\"Predicted\", \"True\"]\n",
    ")\n",
    "print(df_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
